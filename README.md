# Working with big data due to chunk method

## English
### 1) Objective and Task Condition:

The file `events.csv` contains a very large volume of data and may not fit into the computer's memory. Therefore, for greater convenience, it is necessary to perform data sampling in such a way as to retain only 5% of random records for each day.

### 2) Tools Used During the Project:
- Python (libraries: Pandas, Numpy, os);
- For loop

### 3) Outcome:
Processed the file with large data, sampled the data, and retained 5% of random records for each day. The same method can be applied to perform various transformations, grouping, and aggregation of large data for different needs.

## Russian
### 1)Цель и условие задачи:

 Файл events.csv содержит очень большой объём данных и может не поместиться в оперативную память компьютера. Поэтому для большего удобства необходимо произвести семплирование данных таким образом, чтобы оставить только 5% случайных записей для каждого дня.

 ### 2)Во время выполнения проекта я применял следующе инструменты:
 - Python (библиотеки Pandas, Numpy, os);
 - Цикл for

 ### 3)Результат:
 Обработал файл с большими данными, семплировал данные, оставил 5% случайных записей для каждого дня. Таким же способом можно выполнеять различные преобразоваия групировки, агрегации больших данных для различных нужд.

 
